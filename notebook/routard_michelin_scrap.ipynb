{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7689197-4d3f-4a02-a236-cc684952d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#import urllib.request\n",
    "#import time\n",
    "from bs4 import BeautifulSoup\n",
    "#from selenium import webdriver\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864ae6a-43a4-4e01-8e11-b07ba87309d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#routard\n",
    "def scrap_routard(continent, country, city):\n",
    "    \"\"\"continent, country, city in FRENCH\"\"\"\n",
    "    response = requests.get(f'https://www.routard.com/fr/guide/{continent}/{country}/{city}')\n",
    "    routard_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = routard_soup.find_all('a', attrs={'href': re.compile(f'guide/{continent}/{country}/{city}')})\n",
    "    for link in links:\n",
    "        print(link.get('href').split(\"/\")[-1])\n",
    "    # scraper les liens de chaque truc pour récup la description rapide ?\n",
    "\n",
    "scrap_routard('europe', 'grece', 'athenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99686c9f-c3bb-46af-be7f-8c33914789aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guide michelin\n",
    "region = 'attica'\n",
    "city = 'athens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6451145a-a560-4967-bd1a-1405b41755cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "COSTS = ['Affordable', 'Mid-Range', 'Premium', 'Luxury']\n",
    "\n",
    "def scrap_michelin(city, region=None,country = None n_pages=1):\n",
    "    # works if n_pages > number of pages on the website\n",
    "    \n",
    "\n",
    "    \n",
    "    assert(n_pages>0), \"n_pages below 0 !\"\n",
    "    data = []\n",
    "    for i in range(n_pages):\n",
    "        if i>0:\n",
    "            url = base_url + f\"/page/{i+1}?sort=distance\"\n",
    "        else:\n",
    "            url = base_url + \"?sort=distance\"\n",
    "        response = requests.get(url)\n",
    "        michelin_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        restaurants = michelin_soup.find_all('div', {'class': \"card__menu-content\"})\n",
    "        if len(restaurants)==0:\n",
    "            break\n",
    "        for resto in restaurants:\n",
    "            resto_a = resto.find('a')\n",
    "            name = resto_a.text.strip()\n",
    "            link = \"https://guide.michelin.com\" + resto_a.get('href')\n",
    "            nb_stars = 0\n",
    "            bib_gourmand = False #bib-gourmand: nos meilleurs rapports qualité-prix\n",
    "            gastronomie_durable = False\n",
    "            gastronomie_durable = False\n",
    "            for span in resto.find_all('span', {'class':'distinction-icon'}):\n",
    "                for distinction in span.find_all('img'):\n",
    "                    src = distinction.get('src')\n",
    "                    if \"star\" in src:\n",
    "                        nb_stars += 1\n",
    "                    elif \"bib-gourmand\" in src:\n",
    "                        bib_gourmand = True\n",
    "                    elif \"gastronomie-durable\" in src:\n",
    "                        gastronomie_durable = True\n",
    "    \n",
    "            resto_div = resto.find_all('div', {'class': \"card__menu-footer--score\"})\n",
    "            tmp = re.sub(r'\\s+', ' ', resto_div[1].text).strip() # to get \"<$/€> · <cuisine>\"\n",
    "            price, cuisine = tmp.split(\" · \")\n",
    "            cost = COSTS[len(price)-1]\n",
    "            address, latitude, longitude = scrap_resto_link(link)\n",
    "            data.append({\n",
    "                \"name\": name,\n",
    "                \"link\": link,\n",
    "                \"stars\": nb_stars,\n",
    "                \"bib-gourmand\": bib_gourmand,\n",
    "                \"cuisine\": cuisine,\n",
    "                \"cost\": cost,\n",
    "                \"address\": address,\n",
    "                \"latitude\":latitude,\n",
    "                \"longitude\": longitude,\n",
    "                \"gastonomie_durable\": gastronomie_durable\n",
    "            })\n",
    "    if len(data)==0:\n",
    "        return None\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def scrap_resto_link(url):\n",
    "    resto_req = requests.get(url)\n",
    "    resto_soup = BeautifulSoup(resto_req.text, 'html.parser')\n",
    "    address = \"\"\n",
    "    for div in resto_soup.find_all('div', {'class':'data-sheet__block--text'}):\n",
    "        text = div.text.strip()\n",
    "        if not '\\n' in text:\n",
    "            address = text\n",
    "            break\n",
    "    latitude, longitude = None, None\n",
    "    for iframe in resto_soup.find_all('iframe'):\n",
    "        i_src = iframe.get('src')\n",
    "        if i_src is None:\n",
    "            continue\n",
    "        if \"maps\" in i_src:\n",
    "            search = re.search(r'q=([-+]?[0-9]*\\.?[0-9]+),([-+]?[0-9]*\\.?[0-9]+)', i_src)\n",
    "            if search:\n",
    "                latitude = float(search.group(1))\n",
    "                longitude = float(search.group(2))\n",
    "    return address, latitude, longitude\n",
    "\n",
    "# print(len(scrap_michelin('athens', 'attica')))\n",
    "# print(len(scrap_michelin('athens', 'attica', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c307b-94fc-429d-97b6-ca05c97dcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scrap_michelin('athens', 'attica', 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e97968f-1e81-4633-bb2c-d071c227d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://guide.michelin.com/fr/fr/osaka-region/restaurants\n",
    "region = \"osaka\"\n",
    "city = \"osaka\"\n",
    "data = scrap_michelin(city, region, 2)\n",
    "if data is None:\n",
    "    data = scrap_michelin(region, 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be009dc1-c72c-47ed-87b9-22f720eb45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern urls:\n",
    "#      same name region/city:\n",
    "#      .../osaka-region/osaka/...\n",
    "#      .../tokyo-region/tokyo/...\n",
    "#      .../new-york-state/new-york/...\n",
    "#      .../shanghai-municipality/shanghai/...\n",
    "#      ...\n",
    "\n",
    "#      region/city\n",
    "#      .../attica/athens/...\n",
    "\n",
    "#      france (region/city):\n",
    "#      .../grand-est/reims/...\n",
    "\n",
    "#      same city in us (idk other countries) (can't find index):\n",
    "#      .../colorado/denver_1261491/...\n",
    "#      .../denver_2892491_noindex/...\n",
    "#      .../pennsylvania/denver_2942583_noindex/...\n",
    "\n",
    "region = \"osaka\"\n",
    "city = \"osaka\"\n",
    "\n",
    "# possible implementation if you are known to be a fdp\n",
    "data = scrap_michelin(city, region)\n",
    "if data is None:\n",
    "    data = scrap_michelin(region)\n",
    "if data is None:\n",
    "    data = scrap_michelin(city, f'{city}-region')\n",
    "if data is None:\n",
    "    data = scrap_michelin(city, f'{city}-state')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809626fa-a1dc-49c7-863c-ff62f0fdd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour ne pas montrer un écran noir mdt 5min avant que ça retourne quoi que ce soit pdt la prez\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(10000000)):\n",
    "    i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50444ad-f971-413b-a18d-4cb9236b1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour scrap plusieurs trucs en mm temps plutôt que de devoir attendre la fin d'un scrap pour en commencer un 2e\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211cc2f-fd1b-4716-b373-d12541876565",
   "metadata": {},
   "source": [
    "## Suppr stop words 13 langues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71068ec5-10e3-4a37-9b38-82495cf3f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113a3e4-90c3-4c44-b8cb-51b7a8cf5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "language_dict = {\n",
    "  'cs': 'czech',\n",
    "  'da': 'danish',\n",
    "  'nl': 'dutch',\n",
    "  'en':'english',\n",
    "  'et': 'estonian',\n",
    "  'fi': 'finnish',\n",
    "  'fr': 'french',\n",
    "  'de': 'german',\n",
    "  'el': 'greek',\n",
    "  'it': 'italian',\n",
    "  'no': 'norwegian',\n",
    "  'pl': 'polish',\n",
    "  'pt': 'portuguese',\n",
    "  'ru': 'russian',\n",
    "  'sl': 'slovene',\n",
    "  'es': 'spanish',\n",
    "  'sv': 'swedish',\n",
    "  'tr': 'turkish'\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize_without_stopwords(text):\n",
    "  try:\n",
    "    language = language_dict[detect(text)]\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered_text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return word_tokenize(filtered_text, language=language)\n",
    "\n",
    "  except Exception as e:\n",
    "    if len(str(e)) == 4:\n",
    "      print(\"[WARNING] language not supported:\", str(e))\n",
    "    else:\n",
    "      print(\"error\", e)\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d59ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'tokyo'\n",
    "region = 'kanto'\n",
    "pays = 'japon'\n",
    "url = f\"https://guide.michelin.com/fr/fr/restaurants?showMap=false&q={city}+{region}+{pays}\"\n",
    "response = requests.get(url)\n",
    "michelin_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "restaurants = michelin_soup.find_all('div', {'class': \"card__menu-content\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21520e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(city, region, country, page=1):\n",
    "    \"\"\"\n",
    "    Construit l'URL en fonction de la présence de city et country dans le DataFrame.\n",
    "    \"\"\"\n",
    "    if city in df['city'].values and country in df['country'].values:\n",
    "        base_url = df.loc[(df['city'] == city) & (df['country'] == country), 'url'].values[0]\n",
    "        url = base_url + f\"/page/{page}?sort=distance\" if page > 1 else base_url + \"?sort=distance\"\n",
    "    else:\n",
    "        url = f\"https://guide.michelin.com/fr/fr/restaurants?showMap=false&q={city}+{region if region else ''}+{country}\"\n",
    "    return url\n",
    "\n",
    "def fetch_restaurant_data(url):\n",
    "    \"\"\"\n",
    "    Récupère les données de restaurants depuis une URL donnée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        restaurants = soup.find_all('div', {'class': \"card__menu-content\"})\n",
    "        return restaurants if restaurants else None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur lors de la requête : {e}\")\n",
    "        return None\n",
    "    \n",
    "def scrap_restaurant(restaurants):\n",
    "    COSTS = ['Affordable', 'Mid-Range', 'Premium', 'Luxury']\n",
    "    data = []\n",
    "    for resto in restaurants:\n",
    "        resto_a = resto.find('a')\n",
    "        name = resto_a.text.strip()\n",
    "        link = \"https://guide.michelin.com\" + resto_a.get('href')\n",
    "        nb_stars = 0\n",
    "        bib_gourmand = False #bib-gourmand: nos meilleurs rapports qualité-prix\n",
    "        gastronomie_durable = False\n",
    "        gastronomie_durable = False\n",
    "        for span in resto.find_all('span', {'class':'distinction-icon'}):\n",
    "            for distinction in span.find_all('img'):\n",
    "                src = distinction.get('src')\n",
    "                if \"star\" in src:\n",
    "                    nb_stars += 1\n",
    "                elif \"bib-gourmand\" in src:\n",
    "                    bib_gourmand = True\n",
    "                elif \"gastronomie-durable\" in src:\n",
    "                    gastronomie_durable = True\n",
    "\n",
    "        resto_div = resto.find_all('div', {'class': \"card__menu-footer--score\"})\n",
    "        tmp = re.sub(r'\\s+', ' ', resto_div[1].text).strip() # to get \"<$/€> · <cuisine>\"\n",
    "        price, cuisine = tmp.split(\" · \")\n",
    "        cost = COSTS[len(price)-1]\n",
    "        address, latitude, longitude = scrap_resto_link(link)\n",
    "        data.append({\n",
    "            \"name\": name,\n",
    "            \"link\": link,\n",
    "            \"stars\": nb_stars,\n",
    "            \"bib-gourmand\": bib_gourmand,\n",
    "            \"cuisine\": cuisine,\n",
    "            \"cost\": cost,\n",
    "            \"address\": address,\n",
    "            \"latitude\":latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"gastonomie_durable\": gastronomie_durable\n",
    "        })\n",
    "    if len(data)==0:\n",
    "        return None\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def scrap_resto_link(url):\n",
    "    resto_req = requests.get(url)\n",
    "    resto_soup = BeautifulSoup(resto_req.text, 'html.parser')\n",
    "    address = \"\"\n",
    "    for div in resto_soup.find_all('div', {'class':'data-sheet__block--text'}):\n",
    "        text = div.text.strip()\n",
    "        if not '\\n' in text:\n",
    "            address = text\n",
    "            break\n",
    "    latitude, longitude = None, None\n",
    "    for iframe in resto_soup.find_all('iframe'):\n",
    "        i_src = iframe.get('src')\n",
    "        if i_src is None:\n",
    "            continue\n",
    "        if \"maps\" in i_src:\n",
    "            search = re.search(r'q=([-+]?[0-9]*\\.?[0-9]+),([-+]?[0-9]*\\.?[0-9]+)', i_src)\n",
    "            if search:\n",
    "                latitude = float(search.group(1))\n",
    "                longitude = float(search.group(2))\n",
    "    return address, latitude, longitude\n",
    "\n",
    "def scrap_michelin(city, region=None, country=None, n_pages=3):\n",
    "    \"\"\"\n",
    "    Fonction principale pour récupérer les informations des restaurants d'une ville.\n",
    "    Retourne un DataFrame consolidé avec toutes les pages scrappées.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('links_michelin_norm.csv')\n",
    "    assert n_pages > 0, \"n_pages doit être supérieur à 0 !\"\n",
    "\n",
    "    # Initialisation d'un DataFrame vide avec les bonnes colonnes\n",
    "    df_restaurants = pd.DataFrame(columns=[\"name\", \"link\", \"stars\", \"bib-gourmand\", \n",
    "                                           \"cuisine\", \"cost\", \"address\", \"latitude\", \n",
    "                                           \"longitude\", \"gastonomie_durable\"])\n",
    "\n",
    "    page = 1\n",
    "\n",
    "    while page <= n_pages:\n",
    "        if city in df['city'].values and country in df['country'].values:\n",
    "            base_url = df.loc[(df['city'] == city) & (df['country'] == country), 'url'].values[0]\n",
    "            url = base_url + f\"/page/{page}?sort=distance\" if page > 1 else base_url + \"?sort=distance\"\n",
    "        else:\n",
    "            url = f\"https://guide.michelin.com/fr/fr/restaurants?showMap=false&q={city}+{region if region else ''}+{country}\"\n",
    "        restaurants = fetch_restaurant_data(url)\n",
    "\n",
    "        if not restaurants:\n",
    "            break  # Stopper si aucun restaurant trouvé\n",
    "\n",
    "        new_data = scrap_restaurant(restaurants)\n",
    "        \n",
    "        if new_data is not None and not new_data.empty:\n",
    "            df_restaurants = pd.concat([df_restaurants, new_data], ignore_index=True)\n",
    "\n",
    "        page += 1\n",
    "        \n",
    "        if len(restaurants) < 20:\n",
    "            break  # Arrêter si moins de 20 restaurants trouvés (fin de la liste)\n",
    "\n",
    "    return df_restaurants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a05a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('links_michelin_norm.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cef02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_michelin('bangkok','','thailande',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32c07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
