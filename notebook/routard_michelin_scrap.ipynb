{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7689197-4d3f-4a02-a236-cc684952d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3864ae6a-43a4-4e01-8e11-b07ba87309d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acropole\n",
      "agora\n",
      "erechtheion\n",
      "jardin-national\n",
      "mont-lycabette\n",
      "musee-archeologique-national\n",
      "musee-de-l-acropole\n",
      "parlement-place-syndagma\n",
      "parthenon\n",
      "quartier-de-plaka\n"
     ]
    }
   ],
   "source": [
    "#routard\n",
    "def scrap_routard(continent, country, city):\n",
    "    \"\"\"continent, country, city in FRENCH\"\"\"\n",
    "    response = requests.get(f'https://www.routard.com/fr/guide/{continent}/{country}/{city}')\n",
    "    routard_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = routard_soup.find_all('a', attrs={'href': re.compile(f'guide/{continent}/{country}/{city}')})\n",
    "    for link in links:\n",
    "        print(link.get('href').split(\"/\")[-1])\n",
    "    # scraper les liens de chaque truc pour récup la description rapide ?\n",
    "\n",
    "scrap_routard('europe', 'grece', 'athenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99686c9f-c3bb-46af-be7f-8c33914789aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guide michelin\n",
    "region = 'attica'\n",
    "city = 'athens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6451145a-a560-4967-bd1a-1405b41755cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "COSTS = ['Affordable', 'Mid-Range', 'Premium', 'Luxury']\n",
    "\n",
    "def scrap_michelin(city, region=None, n_pages=1):\n",
    "    # works if n_pages > number of pages on the website\n",
    "    if region is None:\n",
    "        base_url = f\"https://guide.michelin.com/fr/fr/{city}-region/restaurants\"\n",
    "    else:\n",
    "        base_url = f\"https://guide.michelin.com/fr/fr/{region}/{city}/restaurants\"\n",
    "    assert(n_pages>0), \"n_pages below 0 !\"\n",
    "    data = []\n",
    "    for i in range(n_pages):\n",
    "        if i>0:\n",
    "            url = base_url + f\"/page/{i+1}?sort=distance\"\n",
    "        else:\n",
    "            url = base_url + \"?sort=distance\"\n",
    "        response = requests.get(url)\n",
    "        michelin_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        restaurants = michelin_soup.find_all('div', {'class': \"card__menu-content\"})\n",
    "        if len(restaurants)==0:\n",
    "            break\n",
    "        for resto in restaurants:\n",
    "            resto_a = resto.find('a')\n",
    "            name = resto_a.text.strip()\n",
    "            link = \"https://guide.michelin.com\" + resto_a.get('href')\n",
    "            nb_stars = 0\n",
    "            bib_gourmand = False #bib-gourmand: nos meilleurs rapports qualité-prix\n",
    "            gastronomie_durable = False\n",
    "            gastronomie_durable = False\n",
    "            for span in resto.find_all('span', {'class':'distinction-icon'}):\n",
    "                for distinction in span.find_all('img'):\n",
    "                    src = distinction.get('src')\n",
    "                    if \"star\" in src:\n",
    "                        nb_stars += 1\n",
    "                    elif \"bib-gourmand\" in src:\n",
    "                        bib_gourmand = True\n",
    "                    elif \"gastronomie-durable\" in src:\n",
    "                        gastronomie_durable = True\n",
    "    \n",
    "            resto_div = resto.find_all('div', {'class': \"card__menu-footer--score\"})\n",
    "            tmp = re.sub(r'\\s+', ' ', resto_div[1].text).strip() # to get \"<$/€> · <cuisine>\"\n",
    "            price, cuisine = tmp.split(\" · \")\n",
    "            cost = COSTS[len(price)-1]\n",
    "            address, latitude, longitude = scrap_resto_link(link)\n",
    "            data.append({\n",
    "                \"name\": name,\n",
    "                \"link\": link,\n",
    "                \"stars\": nb_stars,\n",
    "                \"bib-gourmand\": bib_gourmand,\n",
    "                \"cuisine\": cuisine,\n",
    "                \"cost\": cost,\n",
    "                \"address\": address,\n",
    "                \"latitude\":latitude,\n",
    "                \"longitude\": longitude,\n",
    "                \"gastonomie_durable\": gastronomie_durable\n",
    "            })\n",
    "    if len(data)==0:\n",
    "        return None\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def scrap_resto_link(url):\n",
    "    resto_req = requests.get(url)\n",
    "    resto_soup = BeautifulSoup(resto_req.text, 'html.parser')\n",
    "    address = \"\"\n",
    "    for div in resto_soup.find_all('div', {'class':'data-sheet__block--text'}):\n",
    "        text = div.text.strip()\n",
    "        if not '\\n' in text:\n",
    "            address = text\n",
    "            break\n",
    "    latitude, longitude = None, None\n",
    "    for iframe in resto_soup.find_all('iframe'):\n",
    "        i_src = iframe.get('src')\n",
    "        if i_src is None:\n",
    "            continue\n",
    "        if \"maps\" in i_src:\n",
    "            search = re.search(r'q=([-+]?[0-9]*\\.?[0-9]+),([-+]?[0-9]*\\.?[0-9]+)', i_src)\n",
    "            if search:\n",
    "                latitude = float(search.group(1))\n",
    "                longitude = float(search.group(2))\n",
    "    return address, latitude, longitude\n",
    "\n",
    "# print(len(scrap_michelin('athens', 'attica')))\n",
    "# print(len(scrap_michelin('athens', 'attica', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49c307b-94fc-429d-97b6-ca05c97dcd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(scrap_michelin('athens', 'attica', 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e97968f-1e81-4633-bb2c-d071c227d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://guide.michelin.com/fr/fr/osaka-region/restaurants\n",
    "region = \"osaka\"\n",
    "city = \"osaka\"\n",
    "data = scrap_michelin(city, region, 2)\n",
    "if data is None:\n",
    "    data = scrap_michelin(region, 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be009dc1-c72c-47ed-87b9-22f720eb45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern urls:\n",
    "#      same name region/city:\n",
    "#      .../osaka-region/osaka/...\n",
    "#      .../tokyo-region/tokyo/...\n",
    "#      .../new-york-state/new-york/...\n",
    "#      .../shanghai-municipality/shanghai/...\n",
    "#      ...\n",
    "\n",
    "#      region/city\n",
    "#      .../attica/athens/...\n",
    "\n",
    "#      france (region/city):\n",
    "#      .../grand-est/reims/...\n",
    "\n",
    "#      same city in us (idk other countries) (can't find index):\n",
    "#      .../colorado/denver_1261491/...\n",
    "#      .../denver_2892491_noindex/...\n",
    "#      .../pennsylvania/denver_2942583_noindex/...\n",
    "\n",
    "region = \"osaka\"\n",
    "city = \"osaka\"\n",
    "\n",
    "# possible implementation if you are known to be a fdp\n",
    "data = scrap_michelin(city, region)\n",
    "if data is None:\n",
    "    data = scrap_michelin(region)\n",
    "if data is None:\n",
    "    data = scrap_michelin(city, f'{city}-region')\n",
    "if data is None:\n",
    "    data = scrap_michelin(city, f'{city}-state')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809626fa-a1dc-49c7-863c-ff62f0fdd102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10000000/10000000 [00:03<00:00, 2983587.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# pour ne pas montrer un écran noir mdt 5min avant que ça retourne quoi que ce soit pdt la prez\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(10000000)):\n",
    "    i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50444ad-f971-413b-a18d-4cb9236b1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour scrap plusieurs trucs en mm temps plutôt que de devoir attendre la fin d'un scrap pour en commencer un 2e\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211cc2f-fd1b-4716-b373-d12541876565",
   "metadata": {},
   "source": [
    "## Suppr stop words 13 langues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71068ec5-10e3-4a37-9b38-82495cf3f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113a3e4-90c3-4c44-b8cb-51b7a8cf5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "language_dict = {\n",
    "  'cs': 'czech',\n",
    "  'da': 'danish',\n",
    "  'nl': 'dutch',\n",
    "  'en':'english',\n",
    "  'et': 'estonian',\n",
    "  'fi': 'finnish',\n",
    "  'fr': 'french',\n",
    "  'de': 'german',\n",
    "  'el': 'greek',\n",
    "  'it': 'italian',\n",
    "  'no': 'norwegian',\n",
    "  'pl': 'polish',\n",
    "  'pt': 'portuguese',\n",
    "  'ru': 'russian',\n",
    "  'sl': 'slovene',\n",
    "  'es': 'spanish',\n",
    "  'sv': 'swedish',\n",
    "  'tr': 'turkish'\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize_without_stopwords(text):\n",
    "  try:\n",
    "    language = language_dict[detect(text)]\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered_text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return word_tokenize(filtered_text, language=language)\n",
    "\n",
    "  except Exception as e:\n",
    "    if len(str(e)) == 4:\n",
    "      print(\"[WARNING] language not supported:\", str(e))\n",
    "    else:\n",
    "      print(\"error\", e)\n",
    "    return word_tokenize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
