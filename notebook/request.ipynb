{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "import unicodedata\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "from difflib import get_close_matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pycountry-convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_parse_page(url):\n",
    "    \"\"\"\n",
    "    Récupère et parse le contenu HTML d'une page web.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        # Récupérer la page\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Lève une exception pour les erreurs HTTP\n",
    "        \n",
    "        # Parser le contenu avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération de l'URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_html_to_txt(soup, filename,dir_path='C:/Users/glenn/OneDrive/Bureau/VScode saves/WebScrapping/Projet'):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu HTML formaté dans un fichier texte.\n",
    "    \"\"\"\n",
    "    path=f'{dir_path}/{filename}.txt'\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(soup.prettify())\n",
    "        print(f\"HTML sauvegardé dans le fichier : {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde du fichier : {e}\")\n",
    "\n",
    "def extract_text_by_class(soup, balise,class_name):\n",
    "    \"\"\"\n",
    "    Récupère tous les textes des balises <span> ayant une classe spécifique.\n",
    "    \"\"\"\n",
    "    # Chercher toutes les balises <span> avec la classe donnée\n",
    "    spans = soup.find_all(balise, class_=class_name)\n",
    "    \n",
    "    # Extraire et retourner le texte\n",
    "    return [span.get_text(strip=True) for span in spans]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lonely planet extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LonelyPlanet_attractions(soup):\n",
    "    \n",
    "    texts = extract_text_by_class(soup,\"span\", \"heading-05 font-semibold\")\n",
    "    df_LonelyPlanet=pd.DataFrame({'Title': texts})\n",
    "    df_LonelyPlanet.insert(0, 'site', 'LonelyPlanet')\n",
    "    df_LonelyPlanet['rank'] = range(len(df_LonelyPlanet))\n",
    "    return df_LonelyPlanet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucket List extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BucketList_attractions(soup):\n",
    "\n",
    "    df_BucketList=pd.DataFrame()\n",
    "    # Trouver toutes les balises <article>\n",
    "    articles = soup.find_all('article', class_='listing-card bg-white shadow-listing')\n",
    "\n",
    "    # Initialiser une liste pour stocker les résultats\n",
    "\n",
    "    for article in articles:\n",
    "        # Extraire le titre de la balise <h2> (nom de l'attraction)\n",
    "        title_tag = article.find('h2', class_='text-2xl md:text-3xl font-bold')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Titre non trouvé'\n",
    "\n",
    "        # Initialiser un dictionnaire pour stocker les informations de l'attraction\n",
    "        attraction_info = {'Title': title}\n",
    "\n",
    "        # Trouver toutes les balises <p> avec les informations sur la durée, l'âge, etc.\n",
    "        p_tags = article.find_all('p', class_='flex items-center space-x-1 text-lg')\n",
    "\n",
    "        for p in p_tags:\n",
    "            # Extraire le nom de la catégorie (par exemple \"Duration\", \"Good for age\", etc.)\n",
    "            label_tag = p.find_all('span')[1]\n",
    "            if label_tag:\n",
    "                label_value = label_tag.get_text(strip=True).split(':')\n",
    "                if len(label_value)==2:\n",
    "                    label=label_value[0]\n",
    "                    value=label_value[1]\n",
    "                    attraction_info[label] = value\n",
    "\n",
    "        # Ajouter l'attraction à la liste des résultats\n",
    "        df_BucketList = pd.concat([df_BucketList, pd.DataFrame([attraction_info])], ignore_index=True)\n",
    "    df_BucketList.insert(0, 'site', 'BucketList')\n",
    "    df_BucketList['rank'] = range(len(df_BucketList))\n",
    "    return df_BucketList\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WorldTravelGuide extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WorldTravelGuide_attractions(soup):\n",
    "\n",
    "    df_WorldTravelGuide=pd.DataFrame()\n",
    "    articles = soup.find_all('div', class_='high')\n",
    "    articles.extend(soup.find_all('div', class_='medium'))\n",
    "\n",
    "    for article in articles:\n",
    "        # Extraire le titre de la balise <h2> (nom de l'attraction)\n",
    "        title_tag = article.find('h3')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Titre non trouvé'\n",
    "\n",
    "        # Initialisation du dictionnaire pour stocker les informations extraites\n",
    "        attraction_info = {'Title': title}\n",
    "\n",
    "        # Extraire la description\n",
    "        description_tag = article.find('p')\n",
    "        if description_tag:\n",
    "            attraction_info['description'] = description_tag.get_text(strip=True)\n",
    "\n",
    "        # Extraire l'adresse\n",
    "        address_tag = article.find('b', string=\"Address: \")\n",
    "        if address_tag:\n",
    "            address = address_tag.find_next('span')\n",
    "            if address:\n",
    "                attraction_info['Address'] = address.get_text(strip=True)\n",
    "\n",
    "        # Extraire les horaires d'ouverture\n",
    "        opening_times_tag = article.find('b', string=\"Opening times: \")\n",
    "        if opening_times_tag:\n",
    "            opening_times = opening_times_tag.find_next('p')\n",
    "            if opening_times:\n",
    "                attraction_info['Opening times'] = opening_times.get_text(strip=True)\n",
    "\n",
    "        # Extraire le site Web\n",
    "        website_tag = article.find('b', string=\"Website: \")\n",
    "        if website_tag:\n",
    "            website = website_tag.find_next('a')\n",
    "            if website and website.get('href'):\n",
    "                attraction_info['Website'] = website.get('href')\n",
    "\n",
    "        # Extraire les frais d'admission\n",
    "        admission_fees_tag = article.find('b', string=\"Admission Fees: \")\n",
    "        if admission_fees_tag:\n",
    "            admission_fees = admission_fees_tag.find_next('p')\n",
    "            if admission_fees:\n",
    "                attraction_info['Admission Fees'] = admission_fees.get_text(strip=True)\n",
    "\n",
    "        # Extraire l'accès handicapé\n",
    "        disabled_access_tag = article.find('b', string=\"Disabled Access: \")\n",
    "        if disabled_access_tag:\n",
    "            #comment récupérer le texte juste après disabled_access_tag\n",
    "            disabled_access_text = disabled_access_tag.next_sibling.strip() if disabled_access_tag.next_sibling else 'Non spécifié'\n",
    "            attraction_info['Disabled Access'] = disabled_access_text\n",
    "        df_WorldTravelGuide = pd.concat([df_WorldTravelGuide, pd.DataFrame([attraction_info])], ignore_index=True)\n",
    "\n",
    "    df_WorldTravelGuide.insert(0, 'site', 'WorldTravelGuide')\n",
    "    df_WorldTravelGuide['rank'] = range(len(df_WorldTravelGuide))\n",
    "    return df_WorldTravelGuide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNTraveler_attractions(soup):\n",
    "    df_CNTraveler=pd.DataFrame()\n",
    "    articles = soup.find_all('div', class_='GallerySlideFigCaption-dOeyTg gWbVWR')\n",
    "    for article in articles:\n",
    "        # Extraire le titre de l'attraction\n",
    "        title_tag = article.find('span',class_='GallerySlideCaptionHedText-iqjOmM jwPuvZ')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Titre non trouvé'\n",
    "        attraction_info = {'Title': title}\n",
    "        description_tag = article.find('p')\n",
    "        if description_tag:\n",
    "            attraction_info['description'] = description_tag.get_text(strip=True)\n",
    "        df_CNTraveler = pd.concat([df_CNTraveler, pd.DataFrame([attraction_info])], ignore_index=True)\n",
    "    df_CNTraveler.insert(0, 'site', 'CNTraveler')\n",
    "    df_CNTraveler['rank'] = range(len(df_CNTraveler))\n",
    "    return df_CNTraveler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Routard_attractions(soup):\n",
    "    df_Routard=pd.DataFrame()\n",
    "    articles = soup.find_all('div', class_='bg-rtd-grey-100 flex h-96 w-60 flex-col rounded-xl p-4')\n",
    "    for article in articles:\n",
    "        # Extraire le titre de l'attraction\n",
    "        title_tag = article.find('h2',class_='group-hover:text-rtd-green my-2 font-semibold')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Titre non trouvé'\n",
    "        attraction_info = {'Title': title}\n",
    "        description_tag = article.find('div', class_='rtd-wysiwyg line-clamp-3')\n",
    "        if description_tag:\n",
    "            attraction_info['description'] = description_tag.get_text(strip=True)\n",
    "        df_Routard = pd.concat([df_Routard, pd.DataFrame([attraction_info])], ignore_index=True)\n",
    "    df_Routard.insert(0, 'site', 'Routard')\n",
    "    df_Routard['rank'] = range(len(df_Routard))\n",
    "    return df_Routard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_location(name, src_lang=\"en\", dest_lang=\"fr\"):\n",
    "\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translation = translator.translate(name, src=src_lang, dest=dest_lang).text\n",
    "        print(translation)\n",
    "        translation= translation.replace(\"'\",\"-\").replace(\" \",\"-\").lower()\n",
    "        #suppression des accents\n",
    "        translation = unicodedata.normalize('NFD', translation)\n",
    "        text = ''.join(char for char in translation if unicodedata.category(char) != 'Mn')\n",
    "    \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {e}\")\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def country_to_continent(country_name):\n",
    "    country_names = [country.name for country in pycountry.countries]\n",
    "    country = get_close_matches(country_name, country_names,n=1)\n",
    "    if len(country) == 1:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country[0])\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name.lower()\n",
    "    else: return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routard_city_to_region(city):\n",
    "    cities_to_regions = {\n",
    "        \"strasbourg\": \"alsace\",\n",
    "        \"bordeaux\": \"aquitaine-bordelais-landes\",\n",
    "        \"rennes\": \"bretagne\",\n",
    "        \"nice\": \"cote-d-azur\",\n",
    "        \"paris\": \"ile-de-france\",\n",
    "        \"montpellier\": \"languedoc-roussillon\",\n",
    "        \"toulouse\": \"midi-toulousain-occitanie\",\n",
    "        \"lille\": \"nord-pas-de-calais\",\n",
    "        \"nantes\": \"pays-de-la-loire\",\n",
    "        \"marseille\": \"provence\"\n",
    "    }\n",
    "    return cities_to_regions[city]\n",
    "\n",
    "def routard_continent(continent):\n",
    "    routard_continent_fr = {\n",
    "        \"europe\":\"europe\",\n",
    "        \"africa\":\"afrique\",\n",
    "        \"north america\":\"ameriques\",\n",
    "        \"south america\":\"ameriques\",\n",
    "        \"asia\":\"asie\",\n",
    "        \"oceania\":\"oceanie\"\n",
    "    }\n",
    "    return routard_continent_fr[continent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(country='france',city='paris',websites_to_call=['Routard','WorldTravelGuide','BucketList','LonelyPlanet','CNTraveler']):\n",
    "    city= str(city).lower().replace(\"'\",\"-\").replace(\" \",\"-\")\n",
    "\n",
    "    continent = country_to_continent(country)\n",
    "    continent_fr = routard_continent(continent)\n",
    "    country_fr = translate_location(country)\n",
    "    city_fr = translate_location(city)\n",
    "\n",
    "    URL_dict={\n",
    "        'LonelyPlanet':f'https://www.lonelyplanet.com/{country}/{city}/attractions',\n",
    "        'BucketList':f'https://www.bucketlisttravels.com/destination/{city}/best-things-to-see-and-do',\n",
    "        'WorldTravelGuide':f'https://www.worldtravelguide.net/guides/{continent}/{country}/{city}/things-to-see/',\n",
    "        'CNTraveler':f'https://www.cntraveler.com/gallery/best-things-to-do-in-{city}',\n",
    "        'Routard':f'https://www.routard.com/fr/guide/top/{continent_fr}/{country_fr}/{city_fr}'\n",
    "    }\n",
    "    if country=='france':\n",
    "        region = routard_city_to_region (city_fr)\n",
    "\n",
    "        URL_dict['Routard']=f'https://www.routard.com/fr/guide/top/{country}/{region}/{city_fr}'\n",
    "\n",
    "    URL_extractor={\n",
    "        'LonelyPlanet_attractions': LonelyPlanet_attractions,\n",
    "        'BucketList_attractions':BucketList_attractions,\n",
    "        'WorldTravelGuide_attractions':WorldTravelGuide_attractions,\n",
    "        'CNTraveler_attractions':CNTraveler_attractions,\n",
    "        'Routard_attractions':Routard_attractions\n",
    "    }\n",
    "    df=pd.DataFrame()\n",
    "    for website in websites_to_call:\n",
    "        soup=fetch_and_parse_page(URL_dict[website])\n",
    "        #save_html_to_txt(soup,f'{website}_{city}')\n",
    "        if soup:\n",
    "            df=pd.concat([df,URL_extractor[f'{website}_attractions'](soup)],ignore_index=True)\n",
    "    return (df)\n",
    "        \n",
    "\n",
    "df_main=main(\"germany\", \"berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP regroupement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Initialiser le traducteur\n",
    "translator = Translator()\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        detected = translator.detect(text)\n",
    "        return detected.lang  # Retourne le code langue (ex: 'fr')\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la détection : {e}\"\n",
    "\n",
    "def translate_to_english(text):\n",
    "    status=False\n",
    "    lang = detect_language(text)\n",
    "    if lang == 'en':\n",
    "        return (text,status)  # Aucun besoin de traduction\n",
    "    try:\n",
    "        translated = translator.translate(text, dest='en')\n",
    "        status=True\n",
    "        return (translated.text,status)\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la traduction : {e}\"\n",
    "\n",
    "# Appliquer la fonction de traduction à la colonne 'Title' et créer une nouvelle colonne 'Title_english'\n",
    "for index in df_main.index:\n",
    "    translation=translate_to_english(df_main.at[index, 'Title'])\n",
    "    df_main.at[index, 'Title_english'] = translation[0]\n",
    "    df_main.at[index, 'VO'] = translation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "country='france'\n",
    "city='paris'\n",
    "# Télécharger les stopwords français\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('french') + stopwords.words('english'))\n",
    "stop_words.update([country, city])\n",
    "\n",
    "def remove_stopwords_and_accents(text):\n",
    "    # Supprimer les accents\n",
    "    text = unidecode(text)\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = re.sub(r'\\W+', ' ', text).lower()\n",
    "    # Supprimer les stopwords\n",
    "\n",
    "    # Ensemble pour suivre les mots déjà rencontrés\n",
    "    unique_words = []  # Liste pour stocker les mots uniques\n",
    "    # Parcourir chaque mot dans le texte\n",
    "    for word in text.split():\n",
    "        # Convertir le mot en minuscules pour ignorer la casse\n",
    "        # Si le mot n'est pas un stop word et qu'il n'a pas encore été rencontré\n",
    "        if word not in stop_words and word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "    # Recomposer la chaîne de mots uniques\n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "# Appliquer la fonction à la colonne 'Title' et créer une nouvelle colonne 'Title_cleaned'\n",
    "df_main['Title_english'] = df_main['Title_english'].apply(remove_stopwords_and_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Fonction personnalisée pour mixer token_sort_ratio et partial_token_sort_ratio\n",
    "def combined_token_sort_ratio(s1, s2, **kwargs):\n",
    "    # Calculer les scores individuels\n",
    "    score_token_sort = fuzz.token_sort_ratio(s1, s2)\n",
    "    score_partial_token_sort = fuzz.partial_token_sort_ratio(s1, s2)\n",
    "    \n",
    "    # Mélanger les scores (par exemple, moyenne pondérée)\n",
    "    return 0.4 * score_token_sort + 0.6 * score_partial_token_sort\n",
    "\n",
    "def extractOne(query, grouped_df, scorer=fuzz.ratio, processor=None, score_cutoff=None):\n",
    "    \"\"\"\n",
    "    Recherche l'élément le plus similaire à une chaîne donnée dans une liste de choix.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): La chaîne de recherche.\n",
    "    choices (iterable): Les choix possibles pour la correspondance.\n",
    "    scorer (callable): Fonction de similarité, par défaut `fuzz.ratio`.\n",
    "    processor (callable): Fonction de prétraitement pour les chaînes (par exemple, str.lower).\n",
    "    score_cutoff (float): Score minimum pour accepter une correspondance.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Le choix le plus proche et son score, ou None si aucun choix n'est au-dessus de `score_cutoff`.\n",
    "    \"\"\"\n",
    "    if processor:\n",
    "        query = processor(query)\n",
    "\n",
    "    best_match = None\n",
    "    best_score = score_cutoff if score_cutoff is not None else 0\n",
    "\n",
    "    for index in grouped_df.index:\n",
    "        choice = grouped_df.loc[index,'Title_english']\n",
    "        # Applique le prétraitement si un processor est défini\n",
    "        processed_choice = processor(choice) if processor else choice\n",
    "        # Calcule le score en utilisant le scorer\n",
    "        score = scorer(query, processed_choice)\n",
    "        # Garde la meilleure correspondance\n",
    "        if score > best_score:\n",
    "            best_match = index\n",
    "            best_score = score\n",
    "\n",
    "    # Retourne le meilleur résultat, ou None si aucun ne dépasse score_cutoff\n",
    "    return (best_match, best_score) if best_match is not None else None\n",
    "\n",
    "# Fonction pour trouver les correspondances similaires\n",
    "def find_similar_groups(df, threshold=80):\n",
    "    df['group']=None\n",
    "    last_group=0\n",
    "    for index in df.index:\n",
    "        #crée un sous df avec les Title_english qui ont une valeur différente de None dans la colonne group et une valeur de site différente de celle de l'index\n",
    "        sub_df=df[(df['group'].notnull()) & (df['site']!=df.loc[index,'site'])]\n",
    "        # Utilisation de la fonction personnalisée comme scorer\n",
    "        match = extractOne(df.loc[index,'Title_english'], sub_df, scorer=combined_token_sort_ratio)\n",
    "        if match and match[1] >= threshold:\n",
    "            df.loc[index,'group']=df.loc[match[0],'group']\n",
    "        else:\n",
    "            df.loc[index,'group']=last_group\n",
    "            last_group+=1\n",
    "    return df\n",
    "\n",
    "df_main = find_similar_groups(df_main, threshold=80).sort_values(by='group', ascending=True, inplace=False)\n",
    "#fonctionne bien sauf pour 'triomph arch' -> à corriger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_groups(df):\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for group in df['group'].unique():\n",
    "        group_df = df[df['group'] == group]\n",
    "        \n",
    "        # Garder le titre le plus court qui a VO == True\n",
    "        filtered_group_df = group_df[group_df['VO'] == True]\n",
    "        if not filtered_group_df.empty and filtered_group_df['Title'].notnull().any():\n",
    "            shortest_title_row = filtered_group_df.loc[filtered_group_df['Title'].str.len().idxmin()]\n",
    "        else:\n",
    "            shortest_title_row = group_df.loc[group_df['Title'].str.len().idxmin()]  # ou un comportement par défaut\n",
    "        \n",
    "        # Fusionner les différentes lignes\n",
    "        merged_row = shortest_title_row.copy()\n",
    "        merged_row['count'] = len(group_df)\n",
    "        merged_row['rank'] = group_df['rank'].mean()\n",
    "        \n",
    "        # Ajouter la ligne fusionnée au DataFrame final\n",
    "        merged_df = pd.concat([merged_df, pd.DataFrame([merged_row])], ignore_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "df_merged = merge_groups(df_main).sort_values(by=['count', 'rank'], ascending=[False, True], inplace=False)\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
